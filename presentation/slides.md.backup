# BLIVA
## A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions

**Authors**: Hu, Xu, Li, Li, Chen, Tu  
**AAAI 2024**

---

## Problem: Text-Rich Visual Understanding

<div style="display: grid; grid-template-columns: 1fr auto 1fr; gap: 15px; align-items: center; padding: 20px; background: #f8f9fa; border-radius: 10px; font-size: 0.75em;">
	<div style="text-align: center;">
		<div style="font-size: 1.3em; margin-bottom: 8px;">❌ <strong style="color: #e74c3c;">Current VLMs</strong></div>
		<ul style="text-align: left; margin: 0; padding-left: 20px; line-height: 1.5;">
			<li>32 fixed query tokens</li>
			<li>Text information lost</li>
		</ul>
	</div>
	<div style="font-size: 2em; color: #3498db;">→</div>
	<div style="text-align: center;">
		<div style="font-size: 1.3em; margin-bottom: 8px;">✓ <strong style="color: #27ae60;">BLIVA</strong></div>
		<ul style="text-align: left; margin: 0; padding-left: 20px; line-height: 1.5;">
			<li>Better projection</li>
			<li>Preserves text details</li>
		</ul>
	</div>
</div>

<p style="margin-top: 20px; font-size: 0.9em;"><strong>Examples:</strong> Documents, charts, memes, screenshots, receipts</p>

---

## BLIVA Architecture

<div class="arch-diagram">
	<div class="arch-box">
		Image<br>
		<span class="dimension">224×224×3</span>
	</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box">
		ViT Encoder<br>
		<span class="dimension">257×768</span>
	</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box">
		Query<br>Projector<br>
		<span class="dimension">32×4096</span>
	</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box">
		Vicuna LLM<br>
		<span class="dimension">7B params</span>
	</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box">
		Response
	</div>
</div>

```python
class BLIVAModel:
    def __init__(self):
        self.vision_encoder = ViTModel()
        self.query_projector = ProjectionMLP()
        self.llm = VicunaModel()
```

---

## Query Projector Innovation

<div style="background: #f8f9fa; padding: 12px; border-radius: 10px; text-align: center; margin: 10px 0; font-size: 0.8em;">
	<strong>Vision Features (768-dim)</strong> → <strong>MLP Layers</strong> → <strong>LLM Space (4096-dim)</strong>
</div>

```python
class ProjectionMLP(nn.Module):
    def __init__(self, input_dim, output_dim):
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 4096),
            nn.GELU(),
            nn.Linear(4096, output_dim)
        )
```

<p style="font-size: 0.85em; margin-top: 10px;"><strong>Purpose:</strong> Projects vision → LLM space, preserves text information, better alignment</p>

---

## Training Process

<div class="flow-diagram">
	<div style="display: flex; justify-content: space-around;">
		<div style="flex: 1; text-align: center; padding: 10px;">
			<strong>Stage 1: Pre-training</strong><br>
			<span style="font-size: 0.8em;">129M image-caption pairs</span>
		</div>
		<div style="flex: 1; text-align: center; padding: 10px;">
			<strong>Stage 2: Instruction Tuning</strong><br>
			<span style="font-size: 0.8em;">InstructBLIP format</span>
		</div>
	</div>
</div>

```python
# Pre-training
loss = contrastive_loss(vision_emb, text_emb)

# Instruction tuning
loss = cross_entropy(model(image, instruction), target)
```

---

## Implementation Example

```python
model = BLIVAModel.from_pretrained("mlpc-lab/BLIVA")
processor = AutoProcessor.from_pretrained("mlpc-lab/BLIVA")

inputs = processor(images=image, text=prompt)
outputs = model.generate(**inputs, max_length=200)
response = processor.decode(outputs[0])
```

<p style="font-size: 0.85em; margin-top: 10px;"><strong>Key Strength:</strong> Handles text-heavy images better than other models</p>

---

## Applications & Results

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; font-size: 0.85em;">
	<div>
		<strong>Benchmarks</strong>
		<ul style="margin-top: 5px; line-height: 1.4;">
			<li>OCR-VQA: 59.5% <span style="color: #27ae60;">(+15%)</span></li>
			<li>TextVQA: 52.7%</li>
			<li>DocVQA: 38.2%</li>
		</ul>
	</div>
	<div>
		<strong>Use Cases</strong>
		<ul style="margin-top: 5px; line-height: 1.4;">
			<li>Document understanding</li>
			<li>Chart analysis</li>
			<li>Meme interpretation</li>
		</ul>
	</div>
</div>

---

## Key Architecture Code

<div class="arch-diagram" style="font-size: 0.65em;">
	<div class="arch-box" style="background: #e8f4f8; padding: 8px 12px;">Image</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box" style="background: #fff3cd; padding: 8px 12px;">ViT</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box" style="background: #d4edda; padding: 8px 12px;">Projector</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box" style="background: #f8d7da; padding: 8px 12px;">Concat</div>
	<div class="arch-arrow">→</div>
	<div class="arch-box" style="background: #d1ecf1; padding: 8px 12px;">LLM</div>
</div>

```python
def forward(self, images, questions):
    visual_features = self.vision_encoder(images)
    visual_embeds = self.query_projector(visual_features)
    
    text_embeds = self.llm.embed_tokens(questions)
    inputs = torch.cat([visual_embeds, text_embeds], dim=1)
    
    return self.llm.generate(inputs)
```

<p style="font-size: 0.8em; margin-top: 8px;"><strong>Flow:</strong> Image → ViT (768-dim) → Projector (4096-dim) → Concat → LLM</p>

---

## Quick Start

```bash
pip install transformers torch pillow
```

```python
from transformers import AutoModel
model = AutoModel.from_pretrained("Salesforce/bliva-vicuna-7b")

# Inference
output = model(processor(image, text))
```

---

## Conclusion

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; font-size: 0.85em;">
	<div style="margin-bottom: 12px;">
		<strong style="font-size: 1.1em;">Key Innovations</strong>
		<ul style="margin: 8px 0 0 20px; line-height: 1.5;">
			<li>Enhanced query projector for text preservation</li>
			<li>15% improvement on OCR-VQA</li>
			<li>Open-source: github.com/mlpc-ucsd/BLIVA</li>
		</ul>
	</div>
	<div style="text-align: center; margin-top: 15px; font-size: 1em; border-top: 1px solid rgba(255,255,255,0.3); padding-top: 12px;">
		<strong>Better document/chart understanding with simpler architecture</strong>
	</div>
</div>